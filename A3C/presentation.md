# 발표: 데이터 병렬화 방식 A3C 구현
![슬라이드1](https://github.com/user-attachments/assets/171e683b-c793-4655-8b4d-be8dd383a6cf)

## Tensorflow, Gym, NumPy 버전 업데이트에 따른 수정 사항
![슬라이드2](https://github.com/user-attachments/assets/06c505c5-c51a-4774-980b-4382f49419a4)

gym v0 -> v1
learn 코드에서:
최신 gym 버전은 스칼라 값이 아니라 튜플을 반환하기 때문에 할당 받는 변수를 이렇게 수정해주셔야 합니다.
결과 저장 파일 확장자명도 이렇게 수정해주셔야 진행 가능합니다.

## 그래디언트 병렬화 방식 A3C 리뷰
![슬라이드3](https://github.com/user-attachments/assets/e4e60220-1386-42bd-8063-4f2ab5788678)

그래디언트 병렬화 A3C를 리뷰해보자면, 각 워커가 그래디언트를 계산하는 구조였습니다.


![슬라이드4](https://github.com/user-attachments/assets/45c467f3-ae6c-4d86-ab22-7760066d08e3)
학습을 실행해보면 각 워커노드가 비동기적으로 샘플 수집을 수행합니다.
저는 최대 에피소드 수를 500으로 설정했는데,
max_episode_num이 500을 초과하기 직전에 while문 조건을 충족했던 워커들은 어쨌든 수집을 끝내야 하므로 에피소드 수가 500이 넘어갔음에도 해당 분량에 대해 마저 학습을 진행한 후 종료되는 현상을 관찰할 수 있었습니다.


![슬라이드5](https://github.com/user-attachments/assets/9d4548a2-dfcf-4638-88c2-51e86e63939d)
학습 결과 그래프


![슬라이드6](https://github.com/user-attachments/assets/245994ae-80c9-4383-8134-a570f47f14f9)
학습을 마친 신경망 파라미터를 불러와 진자를 세우는 에이전트를 구동한 결과입니다.


![슬라이드7](https://github.com/user-attachments/assets/f95c388f-ced2-4d40-9e51-e60890bf19a2)
주어진 코드에서 이미지를 생성할 수 있는 패키지를 임포트한 후 gym에서 렌더링 옵션을 주면 애니메이션으로 시각화한 결과를 gif 파일로 확인할 수 있다 하여 진행하였습니다.
시간이 흐름에 따라 각도가 0으로 수렴하므로 진자가 성공적으로 기립합니다.


## Gradient Parallelism 방식 A3C 구조
![슬라이드8](https://github.com/user-attachments/assets/334137b7-2c49-4da6-943d-45584d225bb3)
5.5절로 넘어가서 이번엔 데이터 병렬화 a3c를 구현해보겠습니다.
각 워커가 그래디언트를 계산해서 글로벌 신경망에게 넘기는 그래디언트 병렬화 방식과 달리,


## Data Parallelism 방식 A3C 구조
![슬라이드9](https://github.com/user-attachments/assets/dccad513-ed19-4365-8038-af95ff438a66)
데이터 병렬화 방식에서 워커는 샘플을 처리 없이 바로 글로벌 신경망에 넘깁니다. 글로벌 신경망은 그래디언트를 계산한 후 신경망 파라미터를 업데이트하게 됩니다.


## 데이터 병렬화 방식 A3C의 pros & cons
![슬라이드10](https://github.com/user-attachments/assets/87edaabe-275d-407a-9b9d-9177b68f1e56)
교재에 데이터 병렬화 방식의 장단점은 소개가 되어 있지 않아 찾아봤는데,
그래디언트 병렬화 방식과 달리 중앙에서 그래디언트를 계산하므로 계산 기준이 일관된 것이 장점이라고 합니다.
다만 글로벌 신경망이 그래디언트 계산까지 하게 되므로, 워커가 많아질수록 계산 요청이 증가하여 병목 위험이 발생하는 단점이 있습니다. 또한 워커는 샘플만 생성해 넘기므로 연산 수행 능력이 낭비됩니다.


## 학습 결과
학습을 글로벌 신경망에서 진행한다는 점만 빼면 이전 절과 동일한 양상으로 진행됩니다.


## 각 방식의 학습 결과 비교
![슬라이드13](https://github.com/user-attachments/assets/d5e1e386-023f-42b6-b250-0b80b70c8590)
두 방식의 학습 결과를 비교해봤는데요.
제 개인 노트북에서 진행했기 때문에 하드웨어가 최적화되어있지 않아 딱히 신뢰도가 높진 않겠으나,
Gradient 방식은 더 높은 성능에 도달한 대신 학습이 불안정하고 진동이 컸습니다.
데이터 방식은 대신 좀 더 안정적이고 일관된 성능을 보였습니다.

## 에이전트 구동 결과
마지막으로 학습을 마친 신경망 파라미터를 불러와 진자를 세우는 에이전트를 구동한 결과입니다.
