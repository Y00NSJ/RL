{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Y00NSJ/RL/blob/main/REINFORCE/implement/cartpole_r.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "DP4lvNcU5s9K"
      },
      "outputs": [],
      "source": [
        "# authored-by : https://github.com/multicore-it/rl/blob/main/codes/cartpole_reinforce.ipynb\n",
        "\n",
        "import numpy as np\n",
        "np.bool8 = np.bool_\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "import gym\n",
        "import os\n",
        "\n",
        "import random as rand\n",
        "\n",
        "\n",
        "class Agent(object):\n",
        "    def __init__(self):\n",
        "      # 프로그램 동작 설정\n",
        "        self.env = gym.make('CartPole-v1')\n",
        "        self.state_size = self.env.observation_space.shape[0]\n",
        "        self.action_size = self.env.action_space.n\n",
        "        self.value_size = 1     # 보상값 크기\n",
        "\n",
        "      # 모델 설정\n",
        "        self.node_num = 12\n",
        "        self.learning_rate = 0.0005\n",
        "        self.epochs_cnt = 5\n",
        "        self.model = self.build_model()\n",
        "\n",
        "      # 학습 설정\n",
        "        self.discount_rate = 0.95\n",
        "        self.penalty = -10\n",
        "\n",
        "      # 반복 설정(에피소드 수)\n",
        "        self.episode_num = 500\n",
        "\n",
        "      # 학습 모니터링 설정\n",
        "        self.moving_avg_size = 20\n",
        "        self.reward_list= []\n",
        "        self.count_list = []\n",
        "        self.moving_avg_list = []\n",
        "\n",
        "      # 데이터 수집 환경: 1에피동안 수집된 데이터 저장할 변수, 행동 결정 모델 호출 시 필요한 dummy\n",
        "        self.states, self.action_matrixs, self.action_probs, self.rewards = [],[],[],[]\n",
        "        self.DUMMY_ACTION_MATRIX, self.DUMMY_REWARD = np.zeros((1,1,self.action_size)), np.zeros((1,1,self.value_size))\n",
        "\n",
        "\n",
        "    class MyModel(tf.keras.Model):      # 사용자 정의 비용 함수 사용하기 위해 상속 및 재정의\n",
        "        def train_step(self, data):     # 사용자 정의 비용 함수: 인공신경망은 이를 감소시키는 방향으로 학습 진행\n",
        "            in_datas, out_actions = data\n",
        "            states, action_matrix, rewards = in_datas[0], in_datas[1], in_datas[2]\n",
        "\n",
        "            with tf.GradientTape() as tape: # 편미분하면서 gradient(=기울기) 구하기 위함(연산 기록중)\n",
        "              # 행동 예측: state를 인공신경망에 입력해 정책(각 행동(좌/우) 할 확률) 출력\n",
        "                y_pred = self([states, action_matrix, rewards], training=True)\n",
        "              # 확률 계산: n번 정책을 [n]의 확률로 선택\n",
        "                action_probs = K.sum(action_matrix*y_pred, axis=-1)\n",
        "              # 비용 함수\n",
        "                loss = -K.log(action_probs)*rewards\n",
        "\n",
        "          # 모델 가중치\n",
        "            trainable_vars = self.trainable_variables\n",
        "          # gradient 호출 및 적용\n",
        "            gradients = tape.gradient(loss, trainable_vars)\n",
        "            self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "            return {\"loss\": tf.reduce_mean(loss)}\n",
        "\n",
        "    def build_model(self):  # 모델 생성\n",
        "        input_states = Input(shape=(1,self.state_size), name='input_states')\n",
        "        input_action_matrixs = Input(shape=(1,self.action_size), name='input_action_matrixs')\n",
        "        input_rewards = Input(shape=(1,self.value_size), name='input_rewards')\n",
        "\n",
        "      # network 구성\n",
        "        x = (input_states)\n",
        "        x = Dense(self.node_num, activation='tanh')(x)\n",
        "        # 행동별 정책에 대한 확률이 출력으로 사용됨 => 활성 함수로 softmax 활용\n",
        "        out_actions = Dense(self.action_size, activation='softmax', name='output')(x)\n",
        "\n",
        "        model = self.MyModel(inputs=[input_states, input_action_matrixs, input_rewards], outputs=out_actions)\n",
        "\n",
        "        model.compile(optimizer=Adam(learning_rate=self.learning_rate))\n",
        "\n",
        "        model.summary()\n",
        "        return model\n",
        "\n",
        "    def train(self): # 1에피 동안 수집한 모든 데이터를 사용해 학습\n",
        "        for episode in range(self.episode_num):\n",
        "\n",
        "            state = self.env.reset()\n",
        "            self.env.max_episode_steps = 500\n",
        "\n",
        "          # 1에피 간 데이터 수집 -> 클래스 변수에 리스트 형식으로 저장\n",
        "            count, reward_tot = self.make_memory(episode, state)\n",
        "          # 수집된 데이터 기반으로 인공신경망 모델 학습\n",
        "            self.train_mini_batch()\n",
        "            self.clear_memory()\n",
        "\n",
        "            if count < 500:\n",
        "                reward_tot = reward_tot-self.penalty\n",
        "\n",
        "            self.reward_list.append(reward_tot)\n",
        "            self.count_list.append(count)\n",
        "            self.moving_avg_list.append(self.moving_avg(self.count_list,self.moving_avg_size))\n",
        "\n",
        "            if(episode % 10 == 0):\n",
        "                print(\"episode:{}, moving_avg:{}, rewards_avg:{}\".format(episode, self.moving_avg_list[-1], np.mean(self.reward_list)))\n",
        "\n",
        "        self.save_model()\n",
        "\n",
        "    def make_memory(self, episode, state):  # 프로그램 실행하며 경험 수집\n",
        "        reward_tot = 0\n",
        "        count = 0\n",
        "        reward = np.zeros(self.value_size)\n",
        "        action_matrix = np.zeros(self.action_size)\n",
        "        done = False\n",
        "        while not done:\n",
        "            count+=1\n",
        "\n",
        "            state_t = np.reshape(state,[1, 1, self.state_size])\n",
        "            action_matrix_t = np.reshape(action_matrix,[1, 1, self.action_size])\n",
        "\n",
        "            action_prob = self.model.predict([state_t, self.DUMMY_ACTION_MATRIX, self.DUMMY_REWARD], verbose=0)\n",
        "\n",
        "            action = np.random.choice(self.action_size, 1, p=action_prob[0][0])[0]\n",
        "            action_matrix = np.zeros(self.action_size)\n",
        "            action_matrix[action] = 1\n",
        "\n",
        "            state_next, reward, done, _ = self.env.step(action)\n",
        "\n",
        "            if count < 500 and done:\n",
        "                reward = self.penalty\n",
        "\n",
        "            self.states.append(np.reshape(state_t, [1,self.state_size]))\n",
        "            self.action_matrixs.append(np.reshape(action_matrix, [1,self.action_size]))\n",
        "            self.action_probs.append(np.reshape(action_prob, [1,self.action_size]))\n",
        "            self.rewards.append(reward)\n",
        "\n",
        "            reward_tot += reward\n",
        "            state = state_next\n",
        "        return count, reward_tot\n",
        "\n",
        "    def clear_memory(self):\n",
        "        self.states, self.action_matrixs, self.action_probs, self.rewards = [],[],[],[] #clear memory\n",
        "\n",
        "\n",
        "    def make_discount_rewards(self, rewards):   # 실행 시점별로 discounted 반환값 계산\n",
        "        discounted_rewards = np.zeros(np.array(rewards).shape)\n",
        "        running_add = 0\n",
        "        for t in reversed(range(0, len(rewards))):\n",
        "            running_add = running_add * self.discount_rate + rewards[t]\n",
        "            discounted_rewards[t] = running_add\n",
        "\n",
        "        return discounted_rewards\n",
        "\n",
        "    def train_mini_batch(self):                 # 수집된 데이터 활용해 모델 학습\n",
        "        if len(self.states) == 0:\n",
        "            #print(\"No data collected. Skipping batch.\")\n",
        "            return\n",
        "      # 1. 단계별 수집된 보상으로 할인된 반환값 계산\n",
        "        discount_rewards = np.array(self.make_discount_rewards(self.rewards)).reshape(-1,1,1)\n",
        "      # 2. 각 데이터를 학습 가능한 형태로 성형\n",
        "        #discount_rewards_t = np.reshape(discount_rewards, [len(discount_rewards),1,1]) # 반환값\n",
        "        # Agent 클래스 변수\n",
        "        states_t = np.array(self.states).reshape(-1, 1, self.state_size)\n",
        "        action_matrixs_t = np.array(self.action_matrixs).reshape(-1, 1, self.action_size)\n",
        "        action_probs_t = np.array(self.action_probs).reshape(-1, self.action_size)\n",
        "\n",
        "        #print(f\"states: {states_t.shape}, actions: {action_matrixs_t.shape}, rewards: {discount_rewards.shape}, probs: {action_probs_t.shape}\")\n",
        "        #print(\"types:\", type(states_t), type(action_matrixs_t), type(discount_rewards), type(action_probs_t))\n",
        "\n",
        "        if (\n",
        "            states_t.shape[0] == 0 or\n",
        "            action_matrixs_t.shape[0] == 0 or\n",
        "            discount_rewards.shape[0] == 0 or\n",
        "            action_probs_t.shape[0] == 0\n",
        "        ):\n",
        "            #print(\"One or more inputs are empty. Skipping batch.\")\n",
        "            return\n",
        "\n",
        "      # 상태, 행동 매트릭스, 반환값 입력; 목표함수로는 행동별 확률을 입력\n",
        "        self.model.fit(x=[states_t, action_matrixs_t, discount_rewards],\n",
        "                       y=action_probs_t,\n",
        "                       epochs=self.epochs_cnt,\n",
        "                       verbose=0)\n",
        "\n",
        "    def moving_avg(self, data, size=10):\n",
        "        if len(data) > size:\n",
        "            c = np.array(data[len(data)-size:len(data)])\n",
        "        else:\n",
        "            c = np.array(data)\n",
        "        return np.mean(c)\n",
        "\n",
        "    def save_model(self):\n",
        "        os.makedirs(\"./model\", exist_ok=True)\n",
        "        self.model.save(\"./model/reinforce.keras\")\n",
        "        print(\"*****end learing\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    agent = Agent()\n",
        "    agent.train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKu2Opsc5s9P"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(agent.reward_list, label='rewards')\n",
        "plt.plot(agent.moving_avg_list, linewidth=4, label='moving average')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('REINFORCE')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkzejzo75s9Q",
        "outputId": "3fc40aa9-f317-4cd7-abd2-8558af1be06e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*action_probs: tf.Tensor([0.6 0.7], shape=(2,), dtype=float64)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "import numpy as np\n",
        "\n",
        "y_pred = np.array([[0.6,0.4],\n",
        "                   [0.3,0.7]])\n",
        "action_matrix = np.array([[1,0],\n",
        "                          [0,1]])\n",
        "\n",
        "action_probs = K.sum(action_matrix*y_pred, axis=-1)\n",
        "print(\"*action_probs:\", action_probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeXsIS4j5s9Q",
        "outputId": "f8e482cc-7fb1-406e-d122-8e7460e7e3ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([2. 4. 6.], shape=(3,), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "import numpy as np\n",
        "\n",
        "x = tf.constant([1.0, 2.0, 3.0])\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    tape.watch(x)\n",
        "    y = (x*x)\n",
        "z = tape.gradient(y, x)\n",
        "print(z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "wpdah65L5s9Q",
        "outputId": "4390915e-09a9-4eb0-d59a-3cd9709090c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(1, 4), (2, 5), (3, 6)]\n"
          ]
        }
      ],
      "source": [
        "b= zip([1, 2, 3], [4, 5, 6])\n",
        "c = list(b)\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zayb0PV5s9Q",
        "outputId": "de1eccca-68c9-450e-ab0a-247cf8fb73c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0, 1, 0, 0, 1, 1, 0, 0, 0, 1, "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "acton_prob = [0.7, 0.3]\n",
        "for i in range(10):\n",
        "    d = np.random.choice(2, 1, p=acton_prob)[0]\n",
        "    print(d, end=', ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLbOBWxj5s9R",
        "outputId": "1427c448-9ffa-4245-fbd6-ea62b55d1771"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4, 3, 2, 1, 0, "
          ]
        }
      ],
      "source": [
        "a = [1,2,3,4,5]\n",
        "for t in reversed(range(0, len(a))):\n",
        "    print(t, end=', ')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}